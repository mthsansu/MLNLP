{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mthsansu/MLNLP/blob/main/Code/Baseline_using_paTable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "ozavBHiqxUvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "4Cbu4nNzJ1Ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ddf612b-a57f-434d-9412-d35a85b47766"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Kx5qouClJu-Z"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.tokenize import TreebankWordTokenizer, TweetTokenizer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import torchtext\n",
        "from sklearn.model_selection import train_test_split\n",
        "from termcolor import colored\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import pyarrow as pa\n",
        "import pyarrow.dataset as ds\n",
        "from datasets import Dataset\n",
        "from torchtext.vocab import GloVe, vocab, FastText\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "cirsbZmuxmvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import data\n",
        "git_url = \"https://raw.githubusercontent.com/mthsansu/MLNLP/main/Data/\"\n",
        "df = pd.read_csv(git_url + 'df_baseline.csv')"
      ],
      "metadata": {
        "id": "CORVZbYGJvuf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the data\n",
        "df = df.drop(columns=['Unnamed: 0'])\n",
        "df = df.rename(columns={'Label': 'label'})"
      ],
      "metadata": {
        "id": "GAJRPLGkKk1I"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "RbsaH_GcKnnn",
        "outputId": "9465f98b-68a0-4e02-c2fb-7ca599fa7be8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  label\n",
              "0  Article de Var Matin, ce jour, sur la proposit...      0\n",
              "1  Je me réjouis de l’annonce du @gouvernementFR ...      1\n",
              "2  Une fois guéris, les enfants hospitalisés au @...      1\n",
              "3  « Progressivement nous devons avoir une foncti...      0\n",
              "4  Réaction commune, avec ma collègue députée Sab...      0\n",
              "5  Solidarité et Compassion pour les victimes et ...      2\n",
              "6  Petit-déjeuner du numérique organisé par Virgi...      0\n",
              "7  L’union nationale aux quatre coins de France <...      0\n",
              "8  Vous ne connaissez pas le sens du mot \"reconqu...      1\n",
              "9  Accords commerciaux: nous avons des intérêts o...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8d7bc768-9b36-40b3-9447-fa0f909fed75\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Article de Var Matin, ce jour, sur la proposit...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Je me réjouis de l’annonce du @gouvernementFR ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Une fois guéris, les enfants hospitalisés au @...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>« Progressivement nous devons avoir une foncti...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Réaction commune, avec ma collègue députée Sab...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Solidarité et Compassion pour les victimes et ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Petit-déjeuner du numérique organisé par Virgi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>L’union nationale aux quatre coins de France &lt;...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Vous ne connaissez pas le sens du mot \"reconqu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Accords commerciaux: nous avons des intérêts o...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8d7bc768-9b36-40b3-9447-fa0f909fed75')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8d7bc768-9b36-40b3-9447-fa0f909fed75 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8d7bc768-9b36-40b3-9447-fa0f909fed75');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data in train, tests and validation sets\n",
        "train, validate, test = \\\n",
        "              np.split(df.sample(frac=1, random_state=42), \n",
        "                       [int(.6*len(df)), int(.8*len(df))])"
      ],
      "metadata": {
        "id": "YVCh_-SlhaCh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained Vectors"
      ],
      "metadata": {
        "id": "VBBLGl-pHq1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Run only once: 5 min long\n",
        "# # fasttext vectors can be imported through torch text (it will download it only once)\n",
        "\n",
        "#pretrained_vectors = GloVe(name=\"6B\", dim='50')\n",
        "#pretrained_vectors = FastText(language='fr')\n",
        "pretrained_vectors = FastText(language='en')"
      ],
      "metadata": {
        "id": "6-VvH4YtHuqz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "7Kuqhd2GGA_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer\n",
        "tok = TweetTokenizer()\n",
        "\n",
        "def tokenize_pad_numericalize(entry, vocab_stoi, max_length=20):\n",
        "  text = [ vocab_stoi[token] if token in vocab_stoi else vocab_stoi['<unk>'] for token in tok.tokenize(entry.lower())]\n",
        "  padded_text = None\n",
        "  if len(text) < max_length:   padded_text = text + [ vocab_stoi['<pad>'] for i in range(len(text), max_length) ] \n",
        "  elif len(text) > max_length: padded_text = text[:max_length]\n",
        "  else:                        padded_text = text\n",
        "  return padded_text\n",
        "\n",
        "def tokenize_all(entries, vocab_stoi):\n",
        "  res = {}\n",
        "  res['text'] = [tokenize_pad_numericalize(entry, vocab_stoi, max_length=200) for entry in entries['text']]\n",
        "  res['label'] = entries['label']\n",
        "  return res"
      ],
      "metadata": {
        "id": "4R4UVndoNjj4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_vocab = vocab(pretrained_vectors.stoi)\n",
        "unk_token = \"<unk>\"\n",
        "unk_index = 0\n",
        "pad_token = '<pad>'\n",
        "pad_index = 1\n",
        "pretrained_vocab.insert_token(\"<unk>\",unk_index)\n",
        "pretrained_vocab.insert_token(\"<pad>\", pad_index)\n",
        "#this is necessary otherwise it will throw runtime error if OOV token is queried \n",
        "pretrained_vocab.set_default_index(unk_index)\n",
        "pretrained_embeddings = pretrained_vectors.vectors\n",
        "pretrained_embeddings = torch.cat((torch.zeros(1,pretrained_embeddings.shape[1]),pretrained_embeddings))\n",
        "pretrained_embeddings.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-r9LBFXKF7hW",
        "outputId": "1fb4cfa5-6178-440b-c656-52ccf4124998"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2519371, 300])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(pa.Table.from_pandas(train))"
      ],
      "metadata": {
        "id": "IHaP-9j9JTKY",
        "outputId": "20fd5f09-0229-4f0f-c740-55bfc4bb3e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-dd383bf7c251>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/typing.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwds)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object.__new__() takes exactly one argument (the type to instantiate)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "A2-852tIKDty",
        "outputId": "11a6a87d-2abb-4399-d947-c6417aef1b16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyarrow._dataset.InMemoryDataset at 0x7fc8508edab0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset(pa.Table.from_pandas(dataset))"
      ],
      "metadata": {
        "id": "eJXHQdzrJ3O-",
        "outputId": "a77e905e-11d1-4266-ab8e-27f8f417f55b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ab084aee21ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    552\u001b[0m      \u001b[0mcolumns_to_convert\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m      \u001b[0mconvert_fields\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_columns_to_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                                                columns)\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;31m# NOTE(wesm): If nthreads=None, then we use a heuristic to decide whether\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_get_columns_to_convert\u001b[0;34m(df, schema, preserve_index, columns)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_columns_to_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_resolve_columns_of_interest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyarrow/pandas_compat.py\u001b[0m in \u001b[0;36m_resolve_columns_of_interest\u001b[0;34m(df, schema, columns)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'pyarrow._dataset.InMemoryDataset' object has no attribute 'columns'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "gU5wjnRXJzJb",
        "outputId": "9c2aa8da-b3c5-465a-94d5-84436ad819dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyarrow._dataset.InMemoryDataset at 0x7fc850615030>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### convert to Huggingface dataset type\n",
        "train_dataset = Dataset(pa.Table.from_pandas(train))\n",
        "test_dataset = Dataset(pa.Table.from_pandas(test))\n",
        "val_dataset = Dataset(pa.Table.from_pandas(validate))\n",
        "\n",
        "# tokenize\n",
        "train_dataset = train_dataset.map(lambda e: tokenize_all(e, pretrained_vocab.get_stoi()), batched=True)\n",
        "test_dataset = test_dataset.map(lambda e: tokenize_all(e, pretrained_vocab.get_stoi()), batched=True)\n",
        "val_dataset = val_dataset.map(lambda e: tokenize_all(e, pretrained_vocab.get_stoi()), batched=True)"
      ],
      "metadata": {
        "id": "KwKatHK-KQeK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "0dd7518b-64af-4ed0-dba8-bfadf86641bb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0a46b96e145f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### convert to Huggingface dataset type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/typing.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, *args, **kwds)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object.__new__() takes exactly one argument (the type to instantiate)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the Dataset class\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, data, args):\n",
        "      self.args = args\n",
        "      self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "      item = {\n",
        "          \"text\": np.array(self.data[idx]['text']),\n",
        "          \"label\": np.array(self.data[idx]['label'])\n",
        "      }\n",
        "      return item"
      ],
      "metadata": {
        "id": "SXji2tSecdfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader\n",
        "args = {'bsize': 64}\n",
        "train_loader = DataLoader(TweetDataset(train_dataset, args), batch_size=args['bsize'], num_workers=2, shuffle=True, drop_last=True)\n",
        "test_loader  = DataLoader(TweetDataset(test_dataset, args), batch_size=args['bsize'], num_workers=2, shuffle=True, drop_last=True)\n",
        "val_loader  = DataLoader(TweetDataset(val_dataset, args), batch_size=args['bsize'], num_workers=2, shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "zuC4MgHbed9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model to classify tweets\n",
        "\n",
        "class TweetModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, pretrained_vectors=None):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(TweetModel, self).__init__()\n",
        "        # apply the pretrained embeddings to transform our token indices, into vectors\n",
        "        self.ebd = torch.nn.Embedding.from_pretrained(pretrained_vectors, freeze=True)\n",
        "        self.hidden_linear_layer = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "        self.classification_layer = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "        # define the dropout strategy (here, 20% (0.2) of the vector is ignored to prevent overfitting)\n",
        "        # we don't use it here but it's a good thing to keep in mind\n",
        "        # self.dropout = nn.Dropout(p=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        # apply the pretrained embeddings\n",
        "        x  = self.ebd(x)\n",
        "        x  = x.mean(1)\n",
        "        h  = torch.relu(self.hidden_linear_layer( x ))\n",
        "        # h  = self.dropout(h)\n",
        "        h  = self.classification_layer(h)\n",
        "        logits = self.softmax(h)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "CP8tPGD4fzkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sizes = next(iter(train_loader))['text'].size()\n",
        "batchsize = sizes[0]\n",
        "inputdim  = sizes[1]\n",
        "print(batchsize, inputdim)"
      ],
      "metadata": {
        "id": "NdtfJXhygGUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hiddendim = 300 # dimension of the pretrained vector\n",
        "outputdim = 3 # because there is 3 classes, i.e. 3 emojis\n",
        "# we instanciate the model\n",
        "tweet_model = TweetModel(inputdim, hiddendim, outputdim, pretrained_vectors=pretrained_vectors.vectors)"
      ],
      "metadata": {
        "id": "5_QeXR2ZgNfj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can look at the model \n",
        "tweet_model"
      ],
      "metadata": {
        "id": "4mugy8DXgSM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda'\n",
        "  print('DEVICE = ', colored(torch.cuda.get_device_name(0), \"green\" ) )\n",
        "else:\n",
        "  device = 'cpu'\n",
        "  print('DEVICE = ', colored('CPU', \"blue\"))\n",
        "tweet_model.to(device)"
      ],
      "metadata": {
        "id": "fZNH_PodgfAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "def train(model, optimizer, ep, args):\n",
        "  # set the model into a training mode : the model's weights and parameters WILL BE updated!\n",
        "  model.train()\n",
        "  # initialize empty lists for losses and accuracies\n",
        "  loss_it, acc_it = list(), list()\n",
        "\n",
        "  # start the loop over all the training batches. This means one full epoch.\n",
        "  for it, batch in tqdm(enumerate(train_loader), desc=\"Epoch %s:\" % (ep), total=train_loader.__len__()):\n",
        "    \n",
        "    batch = {'text': batch['text'].to(device), 'label': batch['label'].to(device)}\n",
        "\n",
        "    # put parameters of the model and the optimizer to zero before doing another iteration. this prevents the gradient accumulation through batches\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # apply the model on the batch\n",
        "    logits = model(batch['text'])\n",
        "\n",
        "    # # # to deal with unbalanced data in the batch, we calculate the weights according to their inverse frequency\n",
        "    b_counter = Counter(batch['label'].detach().cpu().tolist())\n",
        "    b_weights = torch.tensor( [ sum(batch['label'].detach().cpu().tolist()) / b_counter[label] if b_counter[label] > 0 else 0 for label in list(range(args['num_class'])) ] )\n",
        "    b_weights = b_weights.to(device)\n",
        "\n",
        "    # we choose the CrossEntropyLoss, suitable for multiclass classification\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
        "    loss_function = nn.CrossEntropyLoss(weight=b_weights)\n",
        "    # loss_function = nn.CrossEntropyLoss()\n",
        "    loss = loss_function(logits, batch['label'])\n",
        "\n",
        "    # compute backpropagation\n",
        "    loss.backward()\n",
        "\n",
        "    # indicate to the optimizer we've done a step\n",
        "    optimizer.step()\n",
        "\n",
        "    # append the value of the loss for the current iteration (it). .item() retrieve the nuclear value as a int/long\n",
        "    loss_it.append(loss.item())\n",
        "\n",
        "    # get the predicted tags using the maximum probability from the softmax\n",
        "    _, tag_seq  = torch.max(logits, 1)\n",
        "    \n",
        "    # Those 3 lines compute the accuracy and then append it the same way as the loss above\n",
        "    correct = (tag_seq.flatten() == batch['label'].flatten()).float().sum()\n",
        "    acc = correct / batch['label'].flatten().size(0)\n",
        "    acc_it.append(acc.item())\n",
        "\n",
        "  # simple averages of losses and accuracies for this epoch\n",
        "  loss_it_avg = sum(loss_it)/len(loss_it)\n",
        "  acc_it_avg = sum(acc_it)/len(acc_it)\n",
        "  \n",
        "  # print useful information about the training progress and scores on this training set's full pass (i.e. 1 epoch)\n",
        "  print(\"Epoch %s/%s : %s : (%s %s) (%s %s)\" % (colored(str(ep), 'blue'),args['max_eps'] , colored('Training', 'blue'), colored('loss', 'cyan'), sum(loss_it)/len(loss_it), colored('acc', 'cyan'), sum(acc_it) / len(acc_it)))\n"
      ],
      "metadata": {
        "id": "_hXhNntzgxVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(target, loader, model):\n",
        "  \"\"\"\n",
        "    Args:\n",
        "      target (str): modify the display, usually either 'validation' or 'test'\n",
        "  \"\"\"\n",
        "\n",
        "  # set the model into a evaluation mode : the model's weights and parameters will NOT be updated!\n",
        "  model.eval()\n",
        "\n",
        "  # intialize empty list to populate later on\n",
        "  loss_it, acc_it, f1_it = list(), list(), list()\n",
        "  # preds = predicted values ; trues = true values .... obviously~\n",
        "  preds, trues = list(), list()\n",
        "\n",
        "  # loop over the loader batches\n",
        "  for it, batch in tqdm(enumerate(loader), desc=\"%s:\" % (target), total=loader.__len__()):\n",
        "    # set an environnement without any gradient. So the tensor gradients are not considered \n",
        "    # (saves a lot of computation and memory, this is one of the many things that makes predicting far less costly than training)\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # put the batch to the correct device\n",
        "      batch = {'text': batch['text'].to(device), 'label': batch['label'].to(device)}\n",
        "\n",
        "      # apply the model\n",
        "      logits = model(batch['text'])\n",
        "\n",
        "      # # to deal with unbalanced data in the batch, we calculate the weights according to their inverse frequency\n",
        "      # b_counter = Counter(batch['label'].detach().cpu().tolist())\n",
        "      # b_weights = torch.tensor( [ sum(batch['label'].detach().cpu().tolist()) / b_counter[label] if b_counter[label] > 0 else 0 for label in list(range(20)) ] )\n",
        "      # b_weights = b_weights.to(device)\n",
        "\n",
        "      # loss_function = nn.CrossEntropyLoss(weight=b_weights)\n",
        "      loss_function = nn.CrossEntropyLoss()\n",
        "      loss = loss_function(logits, batch['label'])\n",
        "\n",
        "      # no need to backward() and other training stuff. Directly store the loss in the list\n",
        "      loss_it.append(loss.item())\n",
        "\n",
        "      # get the predicted tags using the maximum probability from the softmax\n",
        "      _, tag_seq  = torch.max(logits, 1)\n",
        "      \n",
        "      # compute the accuracy and store it\n",
        "      correct = (tag_seq.flatten() == batch['label'].flatten()).float().sum()\n",
        "      acc = correct / batch['label'].flatten().size(0)\n",
        "      acc_it.append(acc.item())\n",
        "      \n",
        "      # extend the predictions and true labels lists so we can compare them later on\n",
        "      # note how we first ensure the tensor are on cpu (.cpu()), then we detach() the gradient from the tensor, before transforming it to a simple python list (.tolist())\n",
        "      preds.extend(tag_seq.cpu().detach().tolist())\n",
        "      trues.extend(batch['label'].cpu().detach().tolist())\n",
        "\n",
        "  # compute the average loss and accuracy accross the iterations (batches)\n",
        "  loss_it_avg = sum(loss_it)/len(loss_it)\n",
        "  acc_it_avg = sum(acc_it)/len(acc_it)\n",
        "  \n",
        "  # print useful information. Important during training as we want to know the performance over the validation set after each epoch\n",
        "  print(\"%s : (%s %s) (%s %s)\" % ( colored(target, 'blue'), colored('loss', 'cyan'), sum(loss_it)/len(loss_it), colored('acc', 'cyan'), sum(acc_it) / len(acc_it)))\n",
        "\n",
        "  # return the true and predicted values with the losses and accuracies\n",
        "  return trues, preds, loss_it_avg, acc_it_avg, loss_it, acc_it"
      ],
      "metadata": {
        "id": "mBf4ICGbg29I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "def run_epochs(model, args):\n",
        "\n",
        "  args['device'] =device\n",
        "  # if args['cuda'] != -1:\n",
        "  #     model.cuda(args['cuda'])\n",
        "  #     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  #     args['device'] = device\n",
        "  #     print(\"device set to %s\" % (device) )\n",
        "\n",
        "  # we set the optimizer as Adam with the learning rate (lr) set in the arguments\n",
        "  # you can look at the different optimizer available here: https://pytorch.org/docs/stable/optim.html\n",
        "  optimizer = optim.Adam(model.parameters(), lr = args['lr'])\n",
        "\n",
        "  # define an empty list to store validation losses for each epoch\n",
        "  val_ep_losses = list()\n",
        "  # iterate over the number of max epochs set in the arguments\n",
        "  for ep in range(args['max_eps']):\n",
        "    # train the model using our defined function\n",
        "    train(model, optimizer, ep, args)\n",
        "    # apply the model for inference using our defined function\n",
        "    trues, preds, val_loss_it_avg, val_acc_it_avg, val_loss_it, val_acc_it = inference(\"validation\", val_loader, model)\n",
        "    # append the validation losses (good losses should normally go down)\n",
        "    val_ep_losses.append(val_loss_it_avg)\n",
        "\n",
        "  # return the list of epoch validation losses in order to use it later to create a plot\n",
        "  return val_ep_losses\n",
        "    "
      ],
      "metadata": {
        "id": "y6_AqJD8g5Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here you can specify if you want a GPU or a CPU by setting the cuda argument as -1 for CPU and another index for GPU. If you only have one GPU, put 0.\n",
        "args.update({'max_eps': 10, 'lr': 0.001, 'device': 'cpu', 'cuda': 0, 'num_class': 3})\n",
        "# 1e-05\n",
        "print('device', device)\n",
        "# Instantiate model with pre-trained glove vectors\n",
        "# model = TweetModel(pretrained_embeddings, args['num_class'], args, dimension=50, freeze_embeddings = True )\n",
        "tweet_model = TweetModel(inputdim, hiddendim, outputdim, pretrained_vectors=pretrained_vectors.vectors)\n",
        "loss_list_val = run_epochs(tweet_model, args)"
      ],
      "metadata": {
        "id": "q6GhkWbTg_YT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_loss(loss_list):\n",
        "  '''\n",
        "  this function creates a plot. a simple curve showing the different values at each steps.\n",
        "  Here we use it to plot the loss so we named it plot_loss, but the same function with different titles could be used to plot accuracies\n",
        "  or other metrics for instance.\n",
        "  \n",
        "  Args:\n",
        "    loss_list (list of floats): list of numerical values\n",
        "  '''\n",
        "  plt.plot(range(len(loss_list)), loss_list)\n",
        "  plt.xlabel('epochs')\n",
        "  # in our model we use Softmax then NLLLoss which means Cross Entropy loss\n",
        "  plt.ylabel('Cross Entropy')\n",
        "  # in our training loop we used an Adam optimizer so we indicate it there\n",
        "  plt.title('lr: {}, optim_alg:{}'.format(args['lr'], 'Adam'))\n",
        "  # let's directly show the plot when calling this function\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dGYEF5tyhFuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's call our function using the list of validation losses to show the plot\n",
        "plot_loss(loss_list_val)"
      ],
      "metadata": {
        "id": "qlKDsHn3iaVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's use our model for inference on the test set. We can short it by naming it \"test our model\"\n",
        "# we set the target as \"test\" to print the correct info, and use the test loader\n",
        "trues, preds, loss_it_avg, acc_it_avg, loss_it, acc_it = inference(\"test\", test_loader, tweet_model)"
      ],
      "metadata": {
        "id": "PxL1xG1gi4y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = [0,1,2]"
      ],
      "metadata": {
        "id": "CZIhd8xHjPGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, plot_roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "\n",
        "cm = confusion_matrix(np.array(trues).flatten(), np.array(preds).flatten())\n",
        "df_cm = pd.DataFrame(cm, index=names, columns=names)\n",
        "# config plot sizes\n",
        "sn.set(font_scale=1)\n",
        "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 8}, cmap='coolwarm', linewidth=0.5, fmt=\"\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yFjcCQLYibwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "Fb_fgJDtjjLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OuCWZ_QW6Xgp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}